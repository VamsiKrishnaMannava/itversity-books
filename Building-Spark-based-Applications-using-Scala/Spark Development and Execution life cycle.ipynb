{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Development and Execution life cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to understand Spark execution life cycle and understand different terms such as executor, executor tasks, driver program and more?\n",
    "\n",
    "* Develop Spark Application – Get Monthly Product Revenue\n",
    "* Build and Deploy\n",
    "* Local mode vs. YARN mode\n",
    "* Quick walk through of Spark UI\n",
    "* YARN deployment modes\n",
    "* Spark Execution Cycle\n",
    "\n",
    "### Develop Spark Application – Get Monthly Product Revenue\n",
    "\n",
    "Let us start with details with respect to problem statement, design and then implementation.\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "Using retail db dataset, we need to compute Monthly Product Revenue for given month.\n",
    "\n",
    "* We need to consider only completed and closed orders to compute revenue.\n",
    "* Also we need to consider only those transactions for a given month passed as argument.\n",
    "* We need to sort the data in descending order by revenue while saving the output to HDFS Path.\n",
    "\n",
    "**Design** \n",
    "\n",
    "Let us see the design for the given Problem Statement.\n",
    "\n",
    "* Filter for orders which fall in the month passed as the argument\n",
    "* Join filtered orders and order_items to get order_item details for a given month\n",
    "* Get revenue for each product_id\n",
    "* We need to read products from the local file system\n",
    "* Convert into RDD and extract product_id and product_name\n",
    "* Join it with aggregated order_items (product_id, revenue)\n",
    "* Get product_name and revenue for each product\n",
    "\n",
    "**Development**\n",
    "\n",
    "Let us create a new project and develop the logic.\n",
    "\n",
    "* Setup Project\n",
    "    * Scala Version: 2.11.8 (on windows, latest of 2.11 in other environments)\n",
    "    * sbt Version: 0.13.x\n",
    "    * JDK: 1.8.x\n",
    "    * Project Name: SparkDemo\n",
    "* Update build.sbt\n",
    "    * typesafe config\n",
    "    * Spark Core\n",
    "* Update application.properties\n",
    "* Develop Logic to compute revenue per product for given month.\n",
    "* Once the project is setup we can launch Scala REPL with Spark as well as typesafe config dependencies using **sbt console**\n",
    "* Once we get the logic, we can update as part of Program called **GetMonthlyProductRevenue**\n",
    "\n",
    "**USING SBT CONSOLE**\n",
    "\n",
    "As part of this topic, we will see how to access sbt console and use it for exploring Spark based APIs.\n",
    "\n",
    "* Go to the working directory of the project.\n",
    "* Run sbt console\n",
    "* We should be able to use typesafe config APIs as well as Spark APIs.\n",
    "* Create Spark Conf and Spark Context objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "val props = ConfigFactory.load()\n",
    "\n",
    "val envProps = props.getConfig(\"devu\")\n",
    "\n",
    "// As part of the video, we have passed dev.\n",
    "// But to make the code compatible with windows, \n",
    "// there are some changes to the code.\n",
    "// Make sure to pass devu in Ubuntu\n",
    "\n",
    "val inputPath = envProps.getString(\"input.base.dir\")\n",
    "\n",
    "val outputPath = envProps.getString(\"output.base.dir\") + \"monthly_product_revenue\"\n",
    "\n",
    "val month = \"2014-01\"\n",
    "\n",
    "val conf = new SparkConf().\n",
    "\n",
    "  setAppName(\"Revenue Per Product for \" + month).\n",
    "  \n",
    "  setMaster(envProps.getString(\"execution.mode\"))\n",
    "  \n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HADOOP CONFIGURATION**\n",
    "\n",
    "Let us see how we can access Hadoop Configuration.\n",
    "\n",
    "* Spark uses HDFS APIs to read files from supported file systems.\n",
    "* As part of Spark dependencies, we get HDFS APIs as well.\n",
    "* We can get Hadoop Configuration using sc.hadoopConfiguration\n",
    "* Using it, we will be able to create FileSystem Object. It will explose APIs such as exists, delete etc.\n",
    "* We can use those to validate as well as manage input and/or output directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ Make sure to run earlier code to create Spark Context\n",
    "\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "val fs = FileSystem.get(sc.hadoopConfiguration)\n",
    "\n",
    "if (!fs.exists(new Path(inputPath))) {\n",
    "\n",
    "  println(\"Input path does not exist\")\n",
    "  \n",
    "} else {\n",
    "\n",
    "  if (fs.exists(new Path(outputPath)))\n",
    "  \n",
    "  \n",
    "fs.delete(new Path(outputPath), true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**READ AND FILTER ORDERS**\n",
    "\n",
    "As we are able to create Spark Context, now let us actually read and manipulate data from orders.\n",
    "\n",
    "* Read data from orders\n",
    "* Use filter and validate for COMPLETE or CLOSED as well as passed month\n",
    "* Use map to extract order_id and hard coded value 1 so that we can use it to join later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Filter for orders which fall in the month passed as argument\n",
    "\n",
    "val orders = inputPath + \"orders\"\n",
    "\n",
    "val ordersFiltered = sc.textFile(orders).\n",
    "\n",
    "  filter(order => {\n",
    "      \n",
    "    order.split(\",\")(1).contains(month) &&\n",
    "      \n",
    "      List(\"COMPLETE\", \"CLOSED\").contains(order.split(\",\")(3))\n",
    "  }).\n",
    "\n",
    "  map(order => (order.split(\",\")(0).toInt, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JOIN ORDERS AND ORDER ITEMS\n",
    "\n",
    "Now let us join order_items with orders and get product_id and order_item_subtotal.\n",
    "\n",
    "* Read data from order_items\n",
    "* Extract order_id, product_id and order_item_subtotal as a tuple.\n",
    "* First element is order_id and second element is nested tuple which contain product_id and order_item_subtotal.\n",
    "* Join the data set with orders filtered using order_id as key.\n",
    "* It will generate RDD of tuples – **(order_id, ((product_id, order_item_subtotal), 1))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Join filtered orders and order_items to get order_item details for a given month\n",
    "\n",
    "// Get revenue for each product_id\n",
    "\n",
    "val orderItems = inputPath + \"order_items\"\n",
    "\n",
    "val revenueByProductId = sc.textFile(orderItems).\n",
    "\n",
    "  map(orderItem => {\n",
    "      \n",
    "    val oi = orderItem.split(\",\")\n",
    "      \n",
    "    (oi(1).toInt, (oi(2).toInt, oi(4).toFloat)\n",
    "     \n",
    "  }).\n",
    "      \n",
    "  join(ordersFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPUTE REVENUE PER PRODUCT ID\n",
    "\n",
    "Now we can extract product_id and order_item_subtotal and compute revenue for each product_id.\n",
    "\n",
    "* We can discard order_id and 1 from the join ouput.\n",
    "* We can use map and get the required information – product_id and order_item_subtotal.\n",
    "* Using reduceByKey, we should be able to compute revenue for each product_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Join filtered orders and order_items to get order_item details for a given month\n",
    "\n",
    "// Get revenue for each product_id\n",
    "\n",
    "val orderItems = inputPath + \"order_items\"\n",
    "\n",
    "val revenueByProductId = sc.textFile(orderItems).\n",
    "\n",
    "  map(orderItem => {\n",
    "      \n",
    "    val oi = orderItem.split(\",\")\n",
    "      \n",
    "    (oi(1).toInt, (oi(2).toInt, oi(4).toFloat))\n",
    "      \n",
    "  }).\n",
    "\n",
    "  join(ordersFiltered).\n",
    "\n",
    "  map(rec => rec._2._1).\n",
    "\n",
    "  reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
